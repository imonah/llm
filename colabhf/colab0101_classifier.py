from transformers import pipeline
from transformers import AutoTokenizer
from transformers import AutoModel
from transformers import AutoModelForSequenceClassification
import torch

classifier = pipeline(
    "sentiment-analysis", 
    model="distilbert-base-uncased-finetuned-sst-2-english"
)
"""
–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–∞–π–ø–ª–∞–π–Ω–æ–≤:

- feature-extraction (–ø—Ä–µ–≤—Ä–∞—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä—ã) - distilbert‚Äëbase‚Äëuncased‚Äëfinetuned‚Äësst‚Äë2‚Äëenglish
- fill-mask - bert-base-cased
- ner (—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π) - dslim/bert-base-NER
- question-answering - distilbert-base-cased-distilled-squad
- sentiment-analysis - distilbert‚Äëbase‚Äëuncased‚Äëfinetuned‚Äësst‚Äë2‚Äëenglish
- summarization - facebook/bart-large-cnn, t5-base
- text-generation - distilgpt2
- translation - Helsinki-NLP/opus-mt-fr-en
- zero-shot-classification - facebook/bart-large-mnli
"""

print(classifier("I've been waiting for a HuggingFace course my whole life."))
print(classifier([
    "I've been waiting for a HuggingFace course my whole life.", 
    "I hate this so much!"
]))

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModel.from_pretrained(checkpoint)

# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø–æ–º–æ—â—å—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.", 
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
"""
{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
–í–µ–∫—Ç–æ—Ä, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã–π –º–æ–¥—É–ª–µ–º Transformer, –æ–±—ã—á–Ω–æ –±–æ–ª—å—à–æ–π. –û–±—ã—á–Ω–æ –æ–Ω –∏–º–µ–µ—Ç —Ç—Ä–∏ –∏–∑–º–µ—Ä–µ–Ω–∏—è:

–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (Batch size): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –∑–∞ –æ–¥–∏–Ω —Ä–∞–∑ (–≤ –Ω–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ - 2).
–î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (Sequence length): –î–ª–∏–Ω–∞ —á–∏—Å–ª–æ–≤–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (–≤ –Ω–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ - 16).
–°–∫—Ä—ã—Ç—ã–π —Ä–∞–∑–º–µ—Ä (Hidden size): –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞ –∫–∞–∂–¥–æ–≥–æ –≤—Ö–æ–¥–∞ –º–æ–¥–µ–ª–∏.
–û –Ω–µ–º –≥–æ–≤–æ—Ä—è—Ç –∫–∞–∫ –æ ‚Äú–º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º‚Äù –∏–∑-–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è. 
–°–∫—Ä—ã—Ç—ã–π —Ä–∞–∑–º–µ—Ä –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º (768 - –æ–±—ã—á–Ω–æ–µ —è–≤–ª–µ–Ω–∏–µ –¥–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π, 
–∞ –≤ –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª—è—Ö –æ–Ω –º–æ–∂–µ—Ç –¥–æ—Å—Ç–∏–≥–∞—Ç—å 3072 –∏ –±–æ–ª–µ–µ).
torch.Size([2, 16, 768])
"""
# –ü—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
outputs = model(**inputs)
print(outputs.last_hidden_state)
print(outputs.last_hidden_state.shape)

model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
"""
–°—É—â–µ—Å—Ç–≤—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä ü§ó Transformers, –∫–∞–∂–¥–∞—è –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏. –í–æ—Ç –Ω–µ–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫:
*==AutoModel,BertModel - –ª—é–±—ã–µ –º–æ–¥–µ–ª–∏ ü§ó Transformers
AutoModel - –û–±—ä–µ–∫—Ç, –≤–æ–∑–≤—Ä–∞—â–∞—é—â–∏–π –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Ç–æ—á–∫–∏. 
–í AutoModel –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∑–Ω–∞—Ç—å –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Ç–æ—á–∫—É, 
—Å –∫–æ—Ç–æ—Ä–æ–π –Ω—É–∂–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è.

*Model (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–∫—Ä—ã—Ç—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π)
*ForCausalLM
*ForMaskedLM
*ForMultipleChoice
*ForQuestionAnswering
*ForSequenceClassification
*ForTokenClassification
–∏ –¥—Ä—É–≥–∏–µ ü§ó
"""
print(model(**inputs))
outputs = model(**inputs)     
print(outputs.logits)
print(outputs.logits.shape)
"""
tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)
torch.Size([2, 2])

–ù–∞—à–∞ –º–æ–¥–µ–ª—å —Å–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–ª–∞ [-1.5607, 1.6123] –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ [ 4.1692, -3.3464] –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ. 
–≠—Ç–æ –Ω–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –∞ –ª–æ–≥–∏—Ç—ã, —Å—ã—Ä—ã–µ, –Ω–µ–Ω–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏, –≤—ã–≤–µ–¥–µ–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–Ω–∏–º —Å–ª–æ–µ–º –º–æ–¥–µ–ª–∏. 
–ß—Ç–æ–±—ã –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∏—Ö –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, –æ–Ω–∏ –¥–æ–ª–∂–Ω—ã –ø—Ä–æ–π—Ç–∏ —á–µ—Ä–µ–∑ —Å–ª–æ–π SoftMax 
(–≤—Å–µ –º–æ–¥–µ–ª–∏ ü§ó Transformers –≤—ã–≤–æ–¥—è—Ç –ª–æ–≥–∏—Ç—ã, –ø–æ—Å–∫–æ–ª—å–∫—É —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –æ–±—ã—á–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç 
–ø–æ—Å–ª–µ–¥–Ω—é—é —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, —Ç–∞–∫—É—é –∫–∞–∫ SoftMax, —Å —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π –ø–æ—Ç–µ—Ä—å, —Ç–∞–∫–æ–π –∫–∞–∫ –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è)
"""
# –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞
predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
print(model.config.id2label)
"""
tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)

–¢–µ–ø–µ—Ä—å –º—ã –≤–∏–¥–∏–º, —á—Ç–æ –º–æ–¥–µ–ª—å —Å–ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–ª–∞ [0.0402, 0.9598] –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è 
–∏ [0.9995, 0.0005] –¥–ª—è –≤—Ç–æ—Ä–æ–≥–æ. –≠—Ç–æ —É–∑–Ω–∞–≤–∞–µ–º—ã–µ –æ—Ü–µ–Ω–∫–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏.

{0: 'NEGATIVE', 1: 'POSITIVE'}
–ú—ã —É—Å–ø–µ—à–Ω–æ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–ª–∏ —Ç—Ä–∏ —ç—Ç–∞–ø–∞ –∫–æ–Ω–≤–µ–π–µ—Ä–∞: 
–ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Å –ø–æ–º–æ—â—å—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤, –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å –∏ –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫—É! 
"""



